"""
Module providing a connector for the Groq API 
"""
import os
import logging
from typing import Dict, Any

from langchain_groq import ChatGroq
from llm.llm_connector import LLMConnector
from config import settings

logger = logging.getLogger(__name__)

class GroqConnector(LLMConnector):
    """
    Connector for Groq's LLM API
    """
    
    def __init__(self, api_key: str = None, model_name: str = "llama3-70b-8192"):
        """
        Initialize the Groq connector
        
        Args:
            api_key: API key for Groq (defaults to settings.GROQ_API_KEY)
            model_name: Name of the Groq model to use
        """
        self.api_key = api_key or settings.GROQ_API_KEY
        if not self.api_key:
            logger.warning("No Groq API key provided")
        
        self.model_name = model_name
        self.client = ChatGroq(
            api_key=self.api_key,
            model_name=self.model_name,
            temperature=0.7,
            max_tokens=2048
        )
        logger.info(f"Initialized GroqConnector with model: {model_name}")
    
    async def generate(
        self, 
        prompt: str, 
        system_message: str = None,
        temperature: float = 0.7, 
        max_tokens: int = 2048,
        **kwargs
    ) -> str:
        """
        Generate a response via Groq
        
        Args:
            prompt: Text prompt for the model
            system_message: System message to guide model behavior
            temperature: Temperature to control creativity (0.0-1.0)
            max_tokens: Maximum number of tokens to generate
            **kwargs: Additional parameters
            
        Returns:
            Text generated by the model
        """
        try:
            logger.debug(f"Generating with Groq model {self.model_name}, prompt length: {len(prompt)}")
            
            # Configure the client with updated parameters
            self.client.temperature = temperature
            self.client.max_tokens = max_tokens
            
            if system_message:
                # Langchain ChatGroq uses a different approach for system messages
                # We need to combine system message and prompt
                combined_prompt = f"System: {system_message}\n\nUser: {prompt}"
                response = self.client.invoke(combined_prompt)
            else:
                response = self.client.invoke(prompt)
            
            return response.content
        
        except Exception as e:
            logger.error(f"Error generating with Groq: {str(e)}")
            raise
    
    async def check_status(self) -> bool:
        """
        Check if the Groq API is available
        
        Returns:
            True if the API is available, False otherwise
        """
        if not self.api_key:
            return False
            
        try:
            # Make a simple request to check if the API is working
            response = self.client.invoke("Hello")
            return True
        except Exception as e:
            logger.error(f"Error checking Groq status: {str(e)}")
            return False
